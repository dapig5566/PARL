{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练数据缩减"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Data:\n",
      "('uid_118412', array([ 1.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "        0.        ,  0.        ,  0.        ,  0.        ,  1.        ,\n",
      "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
      "        0.        ,  0.        , -0.00130648, -0.01633448]), array([0., 1., 0., 0., 0., 0., 0.]), datetime.date(2022, 8, 1), 1, False, False, (4, 0.3261117316892494))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "\n",
    "data_path = os.path.join(\"data\", \"dataset2train_v2.pkl\")\n",
    "with open(data_path, \"rb\") as f:\n",
    "    check_data = pkl.load(f)\n",
    "\n",
    "print(\"Sample Data:\")\n",
    "print(check_data[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aug:\n",
      "ADV order_num: 1194 / total num: 29625 = 0.04030379746835443\n",
      "redelv num: 1681 / total num: 29625 = 0.9432573839662447\n",
      "ADV redelv num: 270 / ADV order_num: 1194 = 0.7738693467336684\n",
      "total level_dist: [3000, 4500, 8385, 10473, 3267], hist: [0.1013 0.1519 0.283  0.3535 0.1103]\n",
      "ADV level_dist:      [28, 112, 504, 429, 121],         hist: [0.0235 0.0938 0.4221 0.3593 0.1013]\n",
      "\n",
      "Sep:\n",
      "ADV order_num: 707 / total num: 35174 = 0.02010007391823506\n",
      "redelv num: 2042 / total num: 35174 = 0.9419457553875021\n",
      "ADV redelv num: 158 / ADV order_num: 707 = 0.7765205091937766\n",
      "total level_dist: [3000, 4500, 10747, 12631, 4296], hist: [0.0853 0.1279 0.3055 0.3591 0.1221]\n",
      "ADV level_dist:      [7, 39, 286, 295, 80],         hist: [0.0099 0.0552 0.4045 0.4173 0.1132]\n",
      "\n",
      "total order_num: 64799\n",
      "total ADV order_num: 1901\n"
     ]
    }
   ],
   "source": [
    "mon_string = [\"Aug\", \"Sep\"]\n",
    "total_adv_order_num = 0\n",
    "total_order_num = 0\n",
    "\n",
    "for mon in range(len(check_data)):\n",
    "    print(\"{}:\".format(mon_string[mon]))\n",
    "    num_adv_orders = 0\n",
    "    num_total = 0\n",
    "    num_redelv = 0\n",
    "    num_adv_redelv = 0\n",
    "    level_dist = {}\n",
    "    adv_level_dist = {}\n",
    "    for day in range(len(check_data[mon])):\n",
    "        for i in range(len(check_data[mon][day])):\n",
    "            num_total += 1\n",
    "            if check_data[mon][day][i][-3]:\n",
    "                num_adv_orders += 1\n",
    "                if check_data[mon][day][i][-2]:\n",
    "                    num_adv_redelv += 1\n",
    "\n",
    "                if check_data[mon][day][i][-1][0] not in adv_level_dist:\n",
    "                    adv_level_dist[check_data[mon][day][i][-1][0]] = 1\n",
    "                else:\n",
    "                    adv_level_dist[check_data[mon][day][i][-1][0]] += 1\n",
    "\n",
    "            elif check_data[mon][day][i][-2]:\n",
    "                num_redelv += 1\n",
    "\n",
    "            if check_data[mon][day][i][-1][0] not in level_dist:\n",
    "                level_dist[check_data[mon][day][i][-1][0]] = 1\n",
    "            else:\n",
    "                level_dist[check_data[mon][day][i][-1][0]] += 1\n",
    "    total_adv_order_num += num_adv_orders\n",
    "    total_order_num += num_total\n",
    "    \n",
    "    print(\"ADV order_num: {} / total num: {} = {}\".format(num_adv_orders, num_total, num_adv_orders/num_total))\n",
    "    print(\"redelv num: {} / total num: {} = {}\".format(num_redelv, num_total, 1-(num_redelv/num_total)))\n",
    "    print(\"ADV redelv num: {} / ADV order_num: {} = {}\".format(num_adv_redelv, num_adv_orders, 1-(num_adv_redelv/num_adv_orders)))\n",
    "    \n",
    "    np.set_printoptions(precision=4)\n",
    "    level_cnt = [level_dist[i] for i in sorted(level_dist)]\n",
    "    level_hist = np.array([i/np.sum(level_cnt) for i in level_cnt])\n",
    "    print(\"total level_dist: {}, hist: {}\".format(level_cnt, level_hist))\n",
    "    adv_level_cnt = [adv_level_dist[i] for i in sorted(adv_level_dist)]\n",
    "    adv_level_hist = np.array([i/np.sum(adv_level_cnt) for i in adv_level_cnt])\n",
    "\n",
    "    print(\"ADV level_dist:      {},         hist: {}\".format(adv_level_cnt, adv_level_hist))\n",
    "    print()\n",
    "print(\"total order_num: {}\".format(total_order_num))\n",
    "print(\"total ADV order_num: {}\".format(total_adv_order_num))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 继续筛选无人车订单"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_data = []\n",
    "for mon_data in check_data:\n",
    "    new_mon_data = []\n",
    "    for day_data in mon_data:\n",
    "        new_day_data = [i for i in day_data if i[-3]]\n",
    "        new_mon_data.append(new_day_data)\n",
    "    new_train_data.append(new_mon_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "30\n",
      "70\n",
      "30\n",
      "49\n",
      "(1, 0.32611173168924956)\n",
      "(4, 0.3261117316892494)\n",
      "(1, 0.740473603892424)\n",
      "(1, 0.740473603892424)\n",
      "(2, 0.7964934609959796)\n",
      "(3, 1.3402043200512643)\n",
      "(3, 1.3402043200512646)\n",
      "(4, 0.7964934609959796)\n",
      "(4, 0.7964934609959796)\n",
      "(1, 1.6407512468739265)\n",
      "(4, 0.3261117316892494)\n",
      "(3, 1.337104202693495)\n",
      "(3, 1.3402043200512646)\n",
      "(4, 0.3261117316892494)\n",
      "(4, 0.7964934609959795)\n",
      "(3, 1.3402043200512643)\n",
      "(1, 0.740473603892424)\n",
      "(4, 0.8811230744056688)\n",
      "(1, 0.7964934609959796)\n",
      "(1, 0.7964934609959796)\n",
      "(3, 0.881123074405669)\n",
      "(2, 1.3402043200512648)\n",
      "(4, 0.3261117316892494)\n",
      "(5, 0.043546946208700185)\n",
      "(2, 0.7964934609959796)\n",
      "(3, 1.219656760841688)\n",
      "(4, 0.3261117316892494)\n",
      "(4, 0.7964934609959796)\n",
      "(4, 0.7964934609959796)\n",
      "(4, 0.37349918295764173)\n",
      "(4, 0.7964934609959795)\n",
      "(3, 0.881123074405669)\n",
      "(2, 1.3490864960246274)\n",
      "(3, 1.219656760841688)\n",
      "(3, 0.881123074405669)\n",
      "(5, 0.12497481033076557)\n",
      "(4, 0.3734991829576419)\n",
      "(3, 1.3490864960246274)\n",
      "(4, 0.3261117316892494)\n",
      "(2, 1.3490864960246274)\n",
      "(5, 0.043546946208700185)\n",
      "(3, 0.881123074405669)\n",
      "(3, 1.219656760841688)\n",
      "(2, 1.3402043200512648)\n",
      "(3, 1.219656760841688)\n",
      "(3, 1.219656760841688)\n",
      "(3, 0.881123074405669)\n",
      "(3, 1.219656760841688)\n",
      "(3, 0.881123074405669)\n",
      "(3, 0.881123074405669)\n",
      "(3, 1.3402043200512646)\n",
      "(3, 1.3490864960246274)\n",
      "(3, 1.3402043200512646)\n",
      "(3, 0.881123074405669)\n",
      "(3, 0.881123074405669)\n",
      "(3, 1.219656760841688)\n",
      "(4, 0.3261117316892494)\n",
      "(3, 1.3490864960246274)\n",
      "(3, 1.3490864960246274)\n",
      "(4, 0.7964934609959796)\n",
      "(3, 1.219656760841688)\n",
      "(3, 1.3490864960246274)\n",
      "(3, 1.3490864960246274)\n",
      "(4, 0.32611173168924956)\n",
      "(3, 1.337104202693495)\n",
      "(3, 1.219656760841688)\n",
      "(3, 1.3402043200512646)\n",
      "(4, 0.7964934609959796)\n",
      "(3, 0.881123074405669)\n",
      "(3, 1.2196567608416877)\n"
     ]
    }
   ],
   "source": [
    "print(len(new_train_data))\n",
    "print(len(new_train_data[0]))\n",
    "print(len(new_train_data[0][0]))\n",
    "print(len(new_train_data[1]))\n",
    "print(len(new_train_data[1][0]))\n",
    "\n",
    "for i in new_train_data[0][0]:\n",
    "    print(i[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(\"data\", \"dataset2train_v3.pkl\"), \"wb\") as f:\n",
    "    pkl.dump(new_train_data, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测新数据集准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "with open(os.path.join(\"data\", \"dataset2train_v3.pkl\"), \"rb\") as f:\n",
    "    new_train_data = pkl.load(f)\n",
    "\n",
    "with open(os.path.join(\"data\", \"dataset2train_v2.pkl\"), \"rb\") as f:\n",
    "    org_train_data = pkl.load(f)\n",
    "\n",
    "with open(os.path.join(\"data\", \"userdata2train_v2.pkl\"), \"rb\") as f:\n",
    "    user_data = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 19.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP0: loss: 3.68823, acc: 0.158, lvl_acc: 0.178\n",
      "EP0 (VAL): loss: 3.53499, acc: 0.27031, lvl_acc: 0.17686\n",
      "New Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 21.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP1: loss: 3.56317, acc: 0.272, lvl_acc: 0.189\n",
      "EP1 (VAL): loss: 3.17239, acc: 0.52510, lvl_acc: 0.19648\n",
      "New Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 20.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP2: loss: 3.26023, acc: 0.484, lvl_acc: 0.228\n",
      "EP2 (VAL): loss: 2.85236, acc: 0.60068, lvl_acc: 0.20801\n",
      "New Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 21.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP3: loss: 3.00482, acc: 0.583, lvl_acc: 0.267\n",
      "EP3 (VAL): loss: 2.65402, acc: 0.64902, lvl_acc: 0.27910\n",
      "New Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 21.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP4: loss: 2.82008, acc: 0.616, lvl_acc: 0.310\n",
      "EP4 (VAL): loss: 2.52915, acc: 0.66826, lvl_acc: 0.34795\n",
      "New Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 21.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP5: loss: 2.69199, acc: 0.631, lvl_acc: 0.356\n",
      "EP5 (VAL): loss: 2.41044, acc: 0.67627, lvl_acc: 0.42275\n",
      "New Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 21.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP6: loss: 2.60739, acc: 0.634, lvl_acc: 0.415\n",
      "EP6 (VAL): loss: 2.36970, acc: 0.68535, lvl_acc: 0.42676\n",
      "New Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 21.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP7: loss: 2.52257, acc: 0.646, lvl_acc: 0.443\n",
      "EP7 (VAL): loss: 2.29318, acc: 0.68066, lvl_acc: 0.48408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 21.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP8: loss: 2.45936, acc: 0.651, lvl_acc: 0.468\n",
      "EP8 (VAL): loss: 2.25689, acc: 0.68154, lvl_acc: 0.49111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 21.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP9: loss: 2.41131, acc: 0.657, lvl_acc: 0.488\n",
      "EP9 (VAL): loss: 2.23390, acc: 0.67178, lvl_acc: 0.52100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 21.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP10: loss: 2.37014, acc: 0.657, lvl_acc: 0.503\n",
      "EP10 (VAL): loss: 2.16151, acc: 0.67910, lvl_acc: 0.55947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 21.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP11: loss: 2.32148, acc: 0.662, lvl_acc: 0.523\n",
      "EP11 (VAL): loss: 2.16847, acc: 0.67754, lvl_acc: 0.51777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 21.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP12: loss: 2.28710, acc: 0.663, lvl_acc: 0.536\n",
      "EP12 (VAL): loss: 2.10589, acc: 0.67480, lvl_acc: 0.57266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 21.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP13: loss: 2.25494, acc: 0.665, lvl_acc: 0.553\n",
      "EP13 (VAL): loss: 2.07079, acc: 0.67373, lvl_acc: 0.59688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 21.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP14: loss: 2.21327, acc: 0.668, lvl_acc: 0.570\n",
      "EP14 (VAL): loss: 2.08925, acc: 0.67734, lvl_acc: 0.58789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 21.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP15: loss: 2.18535, acc: 0.670, lvl_acc: 0.579\n",
      "EP15 (VAL): loss: 2.03369, acc: 0.68174, lvl_acc: 0.61738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 21.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP16: loss: 2.16004, acc: 0.672, lvl_acc: 0.585\n",
      "EP16 (VAL): loss: 1.97429, acc: 0.68174, lvl_acc: 0.63223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 21.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP17: loss: 2.12744, acc: 0.673, lvl_acc: 0.599\n",
      "EP17 (VAL): loss: 1.96159, acc: 0.68447, lvl_acc: 0.61904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 21.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP18: loss: 2.11080, acc: 0.678, lvl_acc: 0.603\n",
      "EP18 (VAL): loss: 1.95764, acc: 0.68105, lvl_acc: 0.63594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 21.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP19: loss: 2.08708, acc: 0.677, lvl_acc: 0.609\n",
      "EP19 (VAL): loss: 1.94117, acc: 0.68076, lvl_acc: 0.63770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 21.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP20: loss: 2.06345, acc: 0.680, lvl_acc: 0.619\n",
      "EP20 (VAL): loss: 1.94574, acc: 0.67568, lvl_acc: 0.63916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 21.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP21: loss: 2.04391, acc: 0.680, lvl_acc: 0.626\n",
      "EP21 (VAL): loss: 1.94981, acc: 0.67979, lvl_acc: 0.64316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 21.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP22: loss: 2.02792, acc: 0.683, lvl_acc: 0.624\n",
      "EP22 (VAL): loss: 1.90904, acc: 0.68262, lvl_acc: 0.65635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 21.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP23: loss: 2.00339, acc: 0.685, lvl_acc: 0.633\n",
      "EP23 (VAL): loss: 1.89017, acc: 0.67666, lvl_acc: 0.65479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 21.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP24: loss: 1.99235, acc: 0.686, lvl_acc: 0.633\n",
      "EP24 (VAL): loss: 1.96110, acc: 0.68164, lvl_acc: 0.63701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 21.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP25: loss: 1.97336, acc: 0.688, lvl_acc: 0.640\n",
      "EP25 (VAL): loss: 1.90190, acc: 0.67793, lvl_acc: 0.65459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 21.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP26: loss: 1.96285, acc: 0.689, lvl_acc: 0.641\n",
      "EP26 (VAL): loss: 1.85173, acc: 0.69238, lvl_acc: 0.66133\n",
      "New Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 21.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP27: loss: 1.94146, acc: 0.688, lvl_acc: 0.647\n",
      "EP27 (VAL): loss: 1.88107, acc: 0.68057, lvl_acc: 0.66367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 21.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP28: loss: 1.93109, acc: 0.689, lvl_acc: 0.649\n",
      "EP28 (VAL): loss: 1.85698, acc: 0.68291, lvl_acc: 0.65781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 21.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP29: loss: 1.91802, acc: 0.697, lvl_acc: 0.649\n",
      "EP29 (VAL): loss: 1.84364, acc: 0.68408, lvl_acc: 0.66514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 21.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP30: loss: 1.89523, acc: 0.695, lvl_acc: 0.652\n",
      "EP30 (VAL): loss: 1.81614, acc: 0.69355, lvl_acc: 0.67559\n",
      "New Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 21.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP31: loss: 1.88785, acc: 0.695, lvl_acc: 0.659\n",
      "EP31 (VAL): loss: 1.85477, acc: 0.67861, lvl_acc: 0.65615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 21.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP32: loss: 1.87190, acc: 0.698, lvl_acc: 0.661\n",
      "EP32 (VAL): loss: 1.82339, acc: 0.68135, lvl_acc: 0.66924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 21.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP33: loss: 1.84991, acc: 0.699, lvl_acc: 0.667\n",
      "EP33 (VAL): loss: 1.81768, acc: 0.68379, lvl_acc: 0.65762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 21.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP34: loss: 1.85057, acc: 0.701, lvl_acc: 0.665\n",
      "EP34 (VAL): loss: 1.79325, acc: 0.68203, lvl_acc: 0.67393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 21.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP35: loss: 1.83636, acc: 0.703, lvl_acc: 0.668\n",
      "EP35 (VAL): loss: 1.81776, acc: 0.68145, lvl_acc: 0.66104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 21.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP36: loss: 1.82555, acc: 0.704, lvl_acc: 0.672\n",
      "EP36 (VAL): loss: 1.81786, acc: 0.68398, lvl_acc: 0.66377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 21.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP37: loss: 1.81269, acc: 0.705, lvl_acc: 0.673\n",
      "EP37 (VAL): loss: 1.78512, acc: 0.68350, lvl_acc: 0.66494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 21.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP38: loss: 1.80186, acc: 0.705, lvl_acc: 0.678\n",
      "EP38 (VAL): loss: 1.81916, acc: 0.68652, lvl_acc: 0.66895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 21.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP39: loss: 1.78242, acc: 0.710, lvl_acc: 0.683\n",
      "EP39 (VAL): loss: 1.78600, acc: 0.67832, lvl_acc: 0.67002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 21.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP40: loss: 1.77879, acc: 0.706, lvl_acc: 0.680\n",
      "EP40 (VAL): loss: 1.79033, acc: 0.68027, lvl_acc: 0.66934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 21.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP41: loss: 1.75914, acc: 0.709, lvl_acc: 0.685\n",
      "EP41 (VAL): loss: 1.78396, acc: 0.68096, lvl_acc: 0.67637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 21.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP42: loss: 1.75392, acc: 0.712, lvl_acc: 0.691\n",
      "EP42 (VAL): loss: 1.81378, acc: 0.67383, lvl_acc: 0.67197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 21.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP43: loss: 1.74255, acc: 0.711, lvl_acc: 0.690\n",
      "EP43 (VAL): loss: 1.78635, acc: 0.68154, lvl_acc: 0.67578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 21.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP44: loss: 1.73726, acc: 0.711, lvl_acc: 0.690\n",
      "EP44 (VAL): loss: 1.77482, acc: 0.67803, lvl_acc: 0.67119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 21.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP45: loss: 1.72193, acc: 0.714, lvl_acc: 0.695\n",
      "EP45 (VAL): loss: 1.77673, acc: 0.68125, lvl_acc: 0.67559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 21.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP46: loss: 1.71315, acc: 0.715, lvl_acc: 0.696\n",
      "EP46 (VAL): loss: 1.74399, acc: 0.68174, lvl_acc: 0.68213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 21.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP47: loss: 1.71303, acc: 0.714, lvl_acc: 0.697\n",
      "EP47 (VAL): loss: 1.76702, acc: 0.67344, lvl_acc: 0.67744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 21.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP48: loss: 1.69301, acc: 0.719, lvl_acc: 0.701\n",
      "EP48 (VAL): loss: 1.75720, acc: 0.68213, lvl_acc: 0.67607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 21.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP49: loss: 1.69103, acc: 0.718, lvl_acc: 0.698\n",
      "EP49 (VAL): loss: 1.72829, acc: 0.69277, lvl_acc: 0.67783\n"
     ]
    }
   ],
   "source": [
    "from policy_model import CDPModel_DQN_with_pretrain, CDPModel_TDQN_with_pretrain\n",
    "from env import OrderEnv2\n",
    "from order_dataset import OrderDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from env import ACTION_SPACE, OBSERVATION_SPACE_LITE\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "timestp = int(time.time())\n",
    "model_name = \"TDQN_ptv808_test\"\n",
    "dir_path = \"weights/pretrain/{}_{}\".format(model_name, timestp)\n",
    "file_name = \"state_dict.pt\"\n",
    "if not os.path.exists(dir_path):\n",
    "    os.mkdir(dir_path)\n",
    "        \n",
    "_device = torch.device(\"cuda:0\")\n",
    "# env = OrderEnv2()\n",
    "\n",
    "model = CDPModel_TDQN_with_pretrain(obs_space=ACTION_SPACE,\n",
    "                                       action_space=OBSERVATION_SPACE_LITE,\n",
    "                                       num_outputs=256,\n",
    "                                       model_config={},\n",
    "                                       name=\"CDP_TDQN_PRETRAIN\",\n",
    "                                       pre_train=True, device=_device, recurrent=False\n",
    "                                       ).to(_device)\n",
    "                                \n",
    "\n",
    "clsf = nn.Linear(256, 7).to(_device)\n",
    "level_est = nn.Linear(256, 5).to(_device)\n",
    "\n",
    "\n",
    "xent_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # optim = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "lr_sched = StepLR(optim, step_size=50, gamma=0.5)\n",
    "train_dataset = OrderDataset(0, split=\"train\", max_length=20000, data=org_train_data, user_data=user_data)\n",
    "data_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "eval_data_loader = DataLoader(OrderDataset(0, max_length=2000, user_list=train_dataset.get_user_list(), split=\"test\", data=org_train_data, user_data=user_data), batch_size=128, shuffle=True)\n",
    "\n",
    "max_acc = -np.inf\n",
    "for i in range(50):\n",
    "    avg_loss = 0\n",
    "    avg_acc = 0\n",
    "    avg_lvl_acc = 0\n",
    "    model.train()\n",
    "    for x, y1, y2 in tqdm(data_loader):\n",
    "        x = [t.to(_device) for t in x]\n",
    "        y1 = y1.to(_device)\n",
    "        y2 = y2.to(_device)\n",
    "        input_dict = {\"obs\": [x]}\n",
    "        feature, _ = model(input_dict)\n",
    "        logits = clsf(feature)\n",
    "        lvl_pred = level_est(feature)\n",
    "        cls_loss = xent_loss_fn(logits, y1)\n",
    "        # cls_loss = 0\n",
    "        lvl_loss = xent_loss_fn(lvl_pred, y2)\n",
    "        # lvl_loss = 0\n",
    "        loss = cls_loss + lvl_loss\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        avg_loss = avg_loss + loss.detach().item()\n",
    "        avg_acc  = avg_acc + torch.mean(torch.eq(torch.argmax(logits, dim=-1), torch.argmax(y1, dim=-1)).float()).detach().item()\n",
    "        avg_lvl_acc  = avg_lvl_acc + torch.mean(torch.eq(torch.argmax(lvl_pred, dim=-1), torch.argmax(y2, dim=-1)).float()).detach().item()\n",
    "    lr_sched.step()\n",
    "    print(\"EP{}: loss: {:.5f}, acc: {:.3f}, lvl_acc: {:.3f}\".format(i, avg_loss/len(data_loader), avg_acc/len(data_loader), avg_lvl_acc/len(data_loader)))\n",
    "\n",
    "    model.eval()\n",
    "    avg_eval_loss = 0\n",
    "    avg_eval_acc = 0\n",
    "    avg_eval_lvl_acc = 0\n",
    "    avg_eval_hard_acc = 0\n",
    "    avg_eval_easy_acc = 0\n",
    "    for eval_x, eval_y1, eval_y2 in eval_data_loader:\n",
    "        eval_x = [t.to(_device) for t in eval_x]\n",
    "        eval_y1 = eval_y1.to(_device)\n",
    "        eval_y2 = eval_y2.to(_device)\n",
    "        input_dict = {\"obs\": [eval_x]}\n",
    "        feature, _ = model(input_dict)\n",
    "        logits = clsf(feature)\n",
    "        lvl_pred = level_est(feature)\n",
    "        cls_loss = xent_loss_fn(logits, eval_y1)\n",
    "        # cls_loss = 0\n",
    "        lvl_loss = xent_loss_fn(lvl_pred, eval_y2)\n",
    "        # lvl_loss = 0\n",
    "        loss = cls_loss + lvl_loss\n",
    "\n",
    "        acc = torch.mean(torch.eq(torch.argmax(logits, dim=-1), torch.argmax(eval_y1, dim=-1)).float()).detach().item()\n",
    "        lvl_acc = torch.mean(torch.eq(torch.argmax(lvl_pred, dim=-1), torch.argmax(eval_y2, dim=-1)).float()).detach().item()\n",
    "        \n",
    "\n",
    "        # hard_ind = [i for i, v in enumerate(torch.argmax(eval_y1, dim=-1).detach().cpu().numpy()) if v == 7]\n",
    "        # hard_logits = torch.argmax(logits, dim=-1).detach().cpu().numpy()\n",
    "        # hard_logits = np.array([hard_logits[i] for i in hard_ind])\n",
    "        # hard_acc = sum(hard_logits == 7)/len(hard_ind)\n",
    "\n",
    "        # easy_ind = [i for i, v in enumerate(torch.argmax(eval_y1, dim=-1).detach().cpu().numpy()) if v != 7] \n",
    "        # easy_logits = torch.argmax(logits, dim=-1).detach().cpu().numpy()\n",
    "        # easy_logits = np.array([easy_logits[i] for i in easy_ind])\n",
    "        # easy_label = torch.argmax(eval_y1, dim=-1).detach().cpu().numpy()\n",
    "        # easy_label = np.array([easy_label[i] for i in easy_ind])\n",
    "        # easy_acc = sum(easy_logits == easy_label)/len(easy_ind)\n",
    "\n",
    "        avg_eval_loss = avg_eval_loss + loss\n",
    "        avg_eval_acc = avg_eval_acc + acc\n",
    "        avg_eval_lvl_acc = avg_eval_lvl_acc + lvl_acc\n",
    "        # avg_eval_hard_acc = avg_eval_hard_acc + hard_acc\n",
    "        # avg_eval_easy_acc = avg_eval_easy_acc + easy_acc\n",
    "    print(\"EP{} (VAL): loss: {:.5f}, acc: {:.5f}, lvl_acc: {:.5f}\".format(i, avg_eval_loss/len(eval_data_loader), avg_eval_acc/len(eval_data_loader), avg_eval_lvl_acc/len(eval_data_loader)))\n",
    "    if avg_eval_acc/len(eval_data_loader) > max_acc:\n",
    "        torch.save(model.state_dict(), os.path.join(dir_path, file_name))\n",
    "        torch.save(clsf.state_dict(), os.path.join(dir_path, \"clsf.pt\"))\n",
    "        torch.save(level_est.state_dict(), os.path.join(dir_path, \"lest.pt\"))\n",
    "        print(\"New Model Saved.\")\n",
    "        max_acc = avg_eval_acc/len(eval_data_loader)\n",
    "    \n",
    "# torch.save(model.state_dict(), os.path.join(dir_path, file_name))\n",
    "# torch.save(clsf.state_dict(), os.path.join(dir_path, \"clsf.pt\"))\n",
    "# torch.save(level_est.state_dict(), os.path.join(dir_path, \"lest.pt\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载模型测试新数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load(os.path.join(dir_path, file_name))\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "state_dict = torch.load(os.path.join(dir_path, \"clsf.pt\"))\n",
    "clsf.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "state_dict = torch.load(os.path.join(dir_path, \"lest.pt\"))\n",
    "level_est.load_state_dict(state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET0 (VAL): loss: 1.69473, acc: 0.73478, lvl_acc: 0.64475\n",
      "DATASET1 (VAL): loss: 1.69707, acc: 0.71220, lvl_acc: 0.65679\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    test_dataset = OrderDataset(i, split=\"all\", max_length=20000, data=new_train_data, user_data=user_data)\n",
    "    data_loader = DataLoader(test_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "    model.eval()\n",
    "    avg_eval_loss = 0\n",
    "    avg_eval_acc = 0\n",
    "    avg_eval_lvl_acc = 0\n",
    "    avg_eval_hard_acc = 0\n",
    "    avg_eval_easy_acc = 0\n",
    "    for eval_x, eval_y1, eval_y2 in data_loader:\n",
    "        eval_x = [t.to(_device) for t in eval_x]\n",
    "        eval_y1 = eval_y1.to(_device)\n",
    "        eval_y2 = eval_y2.to(_device)\n",
    "        input_dict = {\"obs\": [eval_x]}\n",
    "        feature, _ = model(input_dict)\n",
    "        logits = clsf(feature)\n",
    "        lvl_pred = level_est(feature)\n",
    "        cls_loss = xent_loss_fn(logits, eval_y1)\n",
    "        # cls_loss = 0\n",
    "        lvl_loss = xent_loss_fn(lvl_pred, eval_y2)\n",
    "        # lvl_loss = 0\n",
    "        loss = cls_loss + lvl_loss\n",
    "\n",
    "        acc = torch.mean(torch.eq(torch.argmax(logits, dim=-1), torch.argmax(eval_y1, dim=-1)).float()).detach().item()\n",
    "        lvl_acc = torch.mean(torch.eq(torch.argmax(lvl_pred, dim=-1), torch.argmax(eval_y2, dim=-1)).float()).detach().item()\n",
    "        \n",
    "\n",
    "        # hard_ind = [i for i, v in enumerate(torch.argmax(eval_y1, dim=-1).detach().cpu().numpy()) if v == 7]\n",
    "        # hard_logits = torch.argmax(logits, dim=-1).detach().cpu().numpy()\n",
    "        # hard_logits = np.array([hard_logits[i] for i in hard_ind])\n",
    "        # hard_acc = sum(hard_logits == 7)/len(hard_ind)\n",
    "\n",
    "        # easy_ind = [i for i, v in enumerate(torch.argmax(eval_y1, dim=-1).detach().cpu().numpy()) if v != 7] \n",
    "        # easy_logits = torch.argmax(logits, dim=-1).detach().cpu().numpy()\n",
    "        # easy_logits = np.array([easy_logits[i] for i in easy_ind])\n",
    "        # easy_label = torch.argmax(eval_y1, dim=-1).detach().cpu().numpy()\n",
    "        # easy_label = np.array([easy_label[i] for i in easy_ind])\n",
    "        # easy_acc = sum(easy_logits == easy_label)/len(easy_ind)\n",
    "\n",
    "        avg_eval_loss = avg_eval_loss + loss\n",
    "        avg_eval_acc = avg_eval_acc + acc\n",
    "        avg_eval_lvl_acc = avg_eval_lvl_acc + lvl_acc\n",
    "        # avg_eval_hard_acc = avg_eval_hard_acc + hard_acc\n",
    "        # avg_eval_easy_acc = avg_eval_easy_acc + easy_acc\n",
    "    print(\"DATASET{} (VAL): loss: {:.5f}, acc: {:.5f}, lvl_acc: {:.5f}\".format(i, avg_eval_loss/len(data_loader), avg_eval_acc/len(data_loader), avg_eval_lvl_acc/len(data_loader)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测新数据集送达时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pred_labels = []\n",
    "for i in range(2):\n",
    "    pred_labels = []\n",
    "    test_dataset = OrderDataset(i, split=\"all\", max_length=200000, data=new_train_data, user_data=user_data)\n",
    "    data_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "    model.eval()\n",
    "    avg_eval_loss = 0\n",
    "    avg_eval_acc = 0\n",
    "    avg_eval_lvl_acc = 0\n",
    "    avg_eval_hard_acc = 0\n",
    "    avg_eval_easy_acc = 0\n",
    "    for eval_x, eval_y1, eval_y2 in data_loader:\n",
    "        eval_x = [t.to(_device) for t in eval_x]\n",
    "        eval_y1 = eval_y1.to(_device)\n",
    "        eval_y2 = eval_y2.to(_device)\n",
    "        input_dict = {\"obs\": [eval_x]}\n",
    "        feature, _ = model(input_dict)\n",
    "        logits = clsf(feature)\n",
    "        labels = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "        \n",
    "        pred_labels.extend(list(labels))\n",
    "    all_pred_labels.append(pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1194"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_pred_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "707"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_pred_labels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 写入预测标签\n",
    "p_label = 0\n",
    "prepared_pred_labels = all_pred_labels[0] + all_pred_labels[1]\n",
    "for mon_data in new_train_data:\n",
    "    for day_data in mon_data:\n",
    "        for i, item in enumerate(day_data):\n",
    "            day_data[i] = item + (prepared_pred_labels[p_label],)\n",
    "            p_label += 1\n",
    "assert p_label == len(all_pred_labels[0]) + len(all_pred_labels[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('uid_015497',\n",
       " array([ 1.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         1.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  1.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        , -0.00227784, -0.01302929]),\n",
       " array([0., 0., 1., 0., 0., 0., 0.]),\n",
       " datetime.date(2022, 8, 1),\n",
       " 2,\n",
       " True,\n",
       " False,\n",
       " (1, 0.32611173168924956),\n",
       " 5)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检查写入情况\n",
    "new_train_data[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_train_data[0][0][0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7264597580220936"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算准确率\n",
    "n_correct = 0\n",
    "for mon_data in new_train_data:\n",
    "    for day_data in mon_data:\n",
    "        for i, item in enumerate(day_data):\n",
    "            if np.argmax(item[2]) == item[-1]:\n",
    "                n_correct += 1\n",
    "n_correct / (len(all_pred_labels[0]) + len(all_pred_labels[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 覆盖新数据集\n",
    "with open(os.path.join(\"data\", \"dataset2train_v3.pkl\"), \"wb\") as f:\n",
    "    pkl.dump(new_train_data, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 存储user embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env import ACTION_SPACE, OBSERVATION_SPACE_LITE\n",
    "from policy_model import CDPModel_TDQN_with_pretrain\n",
    "import torch\n",
    "model = CDPModel_TDQN_with_pretrain(obs_space=ACTION_SPACE,\n",
    "                                       action_space=OBSERVATION_SPACE_LITE,\n",
    "                                       num_outputs=256,\n",
    "                                       model_config={},\n",
    "                                       name=\"CDP_TDQN_PRETRAIN\",\n",
    "                                       pre_train=True, device=\"cpu\", recurrent=False\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load(\"weights/pretrain/TDQN_ptv808_test_1691451460/state_dict.pt\")\n",
    "model.load_state_dict(state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os \n",
    "torch.save(model.user_embedding.state_dict(), os.path.join(\"weights/pretrain/TDQN_ptv808_test_1691451460\", \"user_embedding.pt\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env_py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
